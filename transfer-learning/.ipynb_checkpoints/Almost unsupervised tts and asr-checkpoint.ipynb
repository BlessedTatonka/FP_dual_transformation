{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-21 16:44:39 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2022-01-21 16:44:39 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2022-01-21 16:44:39 nmse_clustering:54] Using eigen decomposition from scipy, upgrade torch to 1.9 or higher for faster clustering\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-01-21 16:44:39 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      '\"sox\" backend is being deprecated. '\n",
      "    \n",
      "[NeMo W 2022-01-21 16:44:40 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import DictConfig\n",
    "import pathlib\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from math import ceil\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import ChainDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nemo.collections.asr.data import audio_to_text_dataset\n",
    "from nemo.collections.asr.data.audio_to_text_dali import DALIOutputs\n",
    "from nemo.collections.asr.losses.ctc import CTCLoss\n",
    "from nemo.collections.asr.metrics.wer import WER\n",
    "from nemo.collections.asr.models.asr_model import ASRModel, ExportableEncDecModel\n",
    "from nemo.collections.asr.parts.mixins import ASRModuleMixin\n",
    "from nemo.collections.asr.parts.preprocessing.perturb import process_augmentations\n",
    "from nemo.core.classes.common import PretrainedModelInfo, typecheck\n",
    "from nemo.core.neural_types import AudioSignal, LabelsType, LengthsType, LogprobsType, NeuralType, SpectrogramType\n",
    "from nemo.utils import logging\n",
    "from nemo.collections.tts.models import FastPitchHifiGanE2EModel\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Reduce logging messages for this notebook\n",
    "from nemo.utils import logging\n",
    "logging.setLevel(logging.ERROR)\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.helpers.helpers import regulate_len\n",
    "from nemo.collections.tts.models import MelGanModel\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from ruamel.yaml import YAML\n",
    "except ModuleNotFoundError:\n",
    "    from ruamel_yaml import YAML\n",
    "\n",
    "from dataset import UnpairedAudioToBPEDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'stt_en_citrinet_256_gamma_0_25'\n",
    "config_name = 'model_config.yaml'\n",
    "yaml = YAML(typ='safe')\n",
    "\n",
    "with open(os.path.join(config_path, config_name)) as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "config['tokenizer']['dir'] = 'citrinet_tokenizer/tokenizer_spe_unigram_v1024'\n",
    "config['tokenizer']['type'] = 'bpe'\n",
    "\n",
    "config['train_ds']['manifest_filepath']=\"../../datasets/LJSpeech-1.1/small_manifest.json\"\n",
    "# config['train_ds']['manifest_filepath']=\"../../datasets/an4/train_manifest.json\"\n",
    "config['train_ds']['batch_size'] = 1\n",
    "config['train_ds']['num_workers'] = 12\n",
    "config['train_ds']['pin_memory'] = True\n",
    "\n",
    "config['validation_ds']['manifest_filepath']=\"../../datasets/LJSpeech-1.1/train_manifest_text.json\"\n",
    "# config['validation_ds']['manifest_filepath']=\"../../datasets/an4/test_manifest.json\"\n",
    "config['validation_ds']['batch_size'] = 1\n",
    "config['validation_ds']['num_workers'] = 12\n",
    "config['validation_ds']['pin_memory'] = True\n",
    "\n",
    "config['spec_augment']['freq_masks'] = 0\n",
    "config['spec_augment']['time_masks'] = 0\n",
    "config['optim']['lr'] = 1e-1\n",
    "config['optim']['name'] = 'novograd'\n",
    "config['optim']['betas'] = [0.8, 0.25]\n",
    "config['optim']['weight_decay'] = 0.001\n",
    "config['optim']['sched']['warmup_steps']=1000\n",
    "config['optim']['sched']['min_lr'] = 0.00001\n",
    "\n",
    "config['tokenizer']['model_path'] = 'stt_en_citrinet_256_gamma_0_25/3d20ebb793c84a64a20c7ad26fc64d62_tokenizer.model'\n",
    "config['tokenizer']['vocab_path'] = 'stt_en_citrinet_256_gamma_0_25/df5191f216004f10a268c44e90fdb63f_vocab.txt'\n",
    "config['tokenizer']['spe_tokenizer_vocab'] = 'stt_en_citrinet_256_gamma_0_25/b774eaac83804907843607272fde21a4_tokenizer.vocab'\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE(cfg=DictConfig(config))\n",
    "\n",
    "# asr_model.setup_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-01-21 16:44:41 fastpitch:345] The train dataloader for FastPitchModel() has shuffle set to False!!!\n",
      "[NeMo E 2022-01-21 16:44:41 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f965c527250>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n",
      "[NeMo E 2022-01-21 16:44:43 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f964eab1450>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n",
      "[NeMo E 2022-01-21 16:44:44 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f9646dc0090>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    }
   ],
   "source": [
    "config_path = 'configs'\n",
    "config_name = 'fastpitch_align.yaml'\n",
    "yaml = YAML(typ='safe')\n",
    "\n",
    "with open(os.path.join(config_path, config_name)) as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "config['model']['train_ds']['manifest_filepath']=\"../../datasets/LJSpeech-1.1/small_manifest.json\"\n",
    "config['model']['train_ds']['batch_size'] = 1\n",
    "config['model']['train_ds']['num_workers'] = 12\n",
    "config['model']['train_ds']['pin_memory'] = True\n",
    "\n",
    "config['model']['validation_ds']['manifest_filepath']=\"../../datasets/LJSpeech-1.1/train_manifest_text.json\"\n",
    "config['model']['validation_ds']['batch_size'] = 1\n",
    "config['model']['validation_ds']['num_workers'] = 12\n",
    "config['model']['validation_ds']['pin_memory'] = True\n",
    "\n",
    "config['model']['optim']['lr'] = 1e-1\n",
    "config['model']['optim']['name'] = 'novograd'\n",
    "config['model']['optim']['betas'] = [0.8, 0.25]\n",
    "config['model']['optim']['weight_decay'] = 0.001\n",
    "config['model']['optim']['sched']['warmup_steps']=1000\n",
    "config['model']['optim']['sched']['min_lr'] = 0.00001\n",
    "\n",
    "spec_gen = FastPitchModel.from_config_dict(DictConfig(config['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = 'conf/hifigan'\n",
    "# config_name = 'hifigan22050.yaml'\n",
    "# yaml = YAML(typ='safe')\n",
    "\n",
    "# with open(os.path.join(config_path, config_name)) as f:\n",
    "#     config = yaml.load(f)\n",
    "\n",
    "# vocoder = HifiGanModel.from_config_dict(DictConfig(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'conf'\n",
    "config_name = 'model_config_melgan.yaml'\n",
    "yaml = YAML(typ='safe')\n",
    "\n",
    "with open(os.path.join(config_path, config_name)) as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "vocoder = MelGanModel.from_config_dict(DictConfig(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MelGanModel(\n",
       "  (audio_to_melspec_precessor): FilterbankFeatures()\n",
       "  (generator): MelGANGenerator(\n",
       "    (melgan): Sequential(\n",
       "      (0): ReflectionPad1d((3, 3))\n",
       "      (1): Conv1d(80, 512, kernel_size=(7,), stride=(1,))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "      (3): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "      (4): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((1, 1))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (5): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((3, 3))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), dilation=(3,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (6): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((9, 9))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), dilation=(9,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (7): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((27, 27))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), dilation=(27,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (8): LeakyReLU(negative_slope=0.2)\n",
       "      (9): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "      (10): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((1, 1))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (11): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((3, 3))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), dilation=(3,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (12): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((9, 9))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), dilation=(9,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (13): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((27, 27))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), dilation=(27,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (14): LeakyReLU(negative_slope=0.2)\n",
       "      (15): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "      (16): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((1, 1))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (17): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((3, 3))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(3,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (18): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((9, 9))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(9,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (19): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((27, 27))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(27,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (20): LeakyReLU(negative_slope=0.2)\n",
       "      (21): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (22): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((1, 1))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (23): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((3, 3))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(3,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (24): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((9, 9))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(9,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (25): ResidualStack(\n",
       "        (stack): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2)\n",
       "          (1): ReflectionPad1d((27, 27))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(27,))\n",
       "          (3): LeakyReLU(negative_slope=0.2)\n",
       "          (4): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (skip_layer): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (26): LeakyReLU(negative_slope=0.2)\n",
       "      (27): ReflectionPad1d((3, 3))\n",
       "      (28): Conv1d(32, 1, kernel_size=(7,), stride=(1,))\n",
       "      (29): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (discriminator): MelGANMultiScaleDiscriminator(\n",
       "    (discriminators): ModuleList(\n",
       "      (0): MelGANDiscriminator(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): ReflectionPad1d((7, 7))\n",
       "            (1): Conv1d(1, 16, kernel_size=(15,), stride=(1,))\n",
       "            (2): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(16, 64, kernel_size=(41,), stride=(4,), padding=(20,), groups=4)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(64, 256, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Conv1d(256, 512, kernel_size=(41,), stride=(4,), padding=(20,), groups=64)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (5): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): MelGANDiscriminator(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): ReflectionPad1d((7, 7))\n",
       "            (1): Conv1d(1, 16, kernel_size=(15,), stride=(1,))\n",
       "            (2): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(16, 64, kernel_size=(41,), stride=(4,), padding=(20,), groups=4)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(64, 256, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Conv1d(256, 512, kernel_size=(41,), stride=(4,), padding=(20,), groups=64)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (5): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): MelGANDiscriminator(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): ReflectionPad1d((7, 7))\n",
       "            (1): Conv1d(1, 16, kernel_size=(15,), stride=(1,))\n",
       "            (2): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(16, 64, kernel_size=(41,), stride=(4,), padding=(20,), groups=4)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(64, 256, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Conv1d(256, 512, kernel_size=(41,), stride=(4,), padding=(20,), groups=64)\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): LeakyReLU(negative_slope=0.2)\n",
       "          )\n",
       "          (5): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooling): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "  )\n",
       "  (loss): MultiResolutionSTFTLoss(\n",
       "    (stft_losses): ModuleList(\n",
       "      (0): STFTLoss(\n",
       "        (spectral_convergence_loss): SpectralConvergenceLoss()\n",
       "        (log_stft_magnitude_loss): LogSTFTMagnitudeLoss()\n",
       "      )\n",
       "      (1): STFTLoss(\n",
       "        (spectral_convergence_loss): SpectralConvergenceLoss()\n",
       "        (log_stft_magnitude_loss): LogSTFTMagnitudeLoss()\n",
       "      )\n",
       "      (2): STFTLoss(\n",
       "        (spectral_convergence_loss): SpectralConvergenceLoss()\n",
       "        (log_stft_magnitude_loss): LogSTFTMagnitudeLoss()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mse_loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.cuda()\n",
    "spec_gen.cuda()\n",
    "vocoder.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: [0.8, 0.99]\n",
       "     eps: 1e-08\n",
       "     lr: 0.1\n",
       "     weight_decay: 0.01\n",
       " ),\n",
       " None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocoder.setup_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_train_start():\n",
    "    asr_model.configure_optimizers()\n",
    "    spec_gen.configure_optimizers()\n",
    "#     vocoder.configure_optimizers()\n",
    "    asr_model.configure_callbacks()\n",
    "    spec_gen.configure_callbacks()\n",
    "    vocoder.configure_callbacks()\n",
    "\n",
    "    asr_model.on_fit_start()\n",
    "    spec_gen.on_fit_start()\n",
    "    vocoder.on_fit_start()\n",
    "    asr_model.setup(\"fit\")\n",
    "    spec_gen.setup(\"fit\")\n",
    "    vocoder.setup(\"fit\")\n",
    "\n",
    "    asr_model.on_pretrain_routine_start()\n",
    "    spec_gen.on_pretrain_routine_start()\n",
    "    vocoder.on_pretrain_routine_start()\n",
    "\n",
    "    asr_model.on_pretrain_routine_end()\n",
    "    spec_gen.on_pretrain_routine_end()\n",
    "    vocoder.on_pretrain_routine_end()\n",
    "\n",
    "    asr_model.on_train_start()\n",
    "    spec_gen.on_train_start()\n",
    "    vocoder.on_train_start()\n",
    "    \n",
    "def on_epoch_start():\n",
    "    asr_model.on_epoch_start()\n",
    "    spec_gen.on_epoch_start()\n",
    "    vocoder.on_epoch_start()\n",
    "    asr_model.on_train_epoch_start()\n",
    "    spec_gen.on_train_epoch_start()\n",
    "    vocoder.on_train_epoch_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_asr_training_step(batch_idx, asr_batch, dataloader_idx=0):\n",
    "    asr_model.on_train_batch_start(asr_batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    asr_model.on_before_batch_transfer(asr_batch, dataloader_idx)\n",
    "    asr_model.transfer_batch_to_device(asr_batch, torch.device(\"cuda\"), dataloader_idx)\n",
    "    asr_model.on_after_batch_transfer(asr_batch, dataloader_idx)\n",
    "\n",
    "    signal, signal_len, transcript, transcript_len = asr_batch\n",
    "    \n",
    "#     print(signal.shape, signal_len.shape)\n",
    "    \n",
    "    signal, signal_len, transcript, transcript_len = signal.cuda(), signal_len.cuda(), \\\n",
    "        transcript.cuda(), transcript_len.cuda()\n",
    "\n",
    "    log_probs, encoded_len, predictions = asr_model.forward(input_signal=signal, input_signal_length=signal_len)\n",
    "\n",
    "    loss = asr_model.loss(\n",
    "        log_probs=log_probs, targets=transcript, input_lengths=encoded_len, target_lengths=transcript_len\n",
    "    )\n",
    "\n",
    "    asr_model.on_before_zero_grad(asr_model._optimizer)\n",
    "    asr_model.optimizer_zero_grad(epoch, batch_idx, asr_model._optimizer, batch_idx)\n",
    "\n",
    "    asr_model.on_before_backward(loss)\n",
    "    asr_model.backward(loss, asr_model._optimizer, batch_idx)\n",
    "    asr_model.on_after_backward()\n",
    "\n",
    "    asr_model.on_before_optimizer_step(asr_model._optimizer, batch_idx)\n",
    "    asr_model.optimizer_step(epoch, batch_idx, asr_model._optimizer, batch_idx)\n",
    "\n",
    "    asr_model.on_train_batch_end(predictions, asr_batch, batch_idx, dataloader_idx)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_spec_gen_vocoder_step(batch_idx, tts_batch, dataloader_idx=0):\n",
    "    spec_gen.on_train_batch_start(tts_batch, batch_idx, dataloader_idx)\n",
    "        \n",
    "    spec_gen.on_before_batch_transfer(tts_batch, dataloader_idx)\n",
    "    spec_gen.transfer_batch_to_device(tts_batch, torch.device(\"cuda\"), dataloader_idx)\n",
    "    spec_gen.on_after_batch_transfer(tts_batch, dataloader_idx)\n",
    "\n",
    "    audio, audio_lens, text, text_lens, attn_prior, pitch, speaker = tts_batch\n",
    "    audio, audio_lens, text, text_lens, attn_prior, pitch, speaker = audio.cuda(), \\\n",
    "        audio_lens.cuda(), text.cuda(), text_lens.cuda(), attn_prior, pitch, speaker\n",
    "    \n",
    "    if pitch is not None:\n",
    "        pitch = pitch.cuda()\n",
    "        \n",
    "    attn_prior, durs, speaker = None, None, None\n",
    "\n",
    "    mels, spec_len = spec_gen.preprocessor(input_signal=audio, length=audio_lens)\n",
    "\n",
    "    mels_pred, _, _, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = \\\n",
    "    spec_gen.forward(\n",
    "        text=text,\n",
    "        durs=durs,\n",
    "        pitch=pitch,\n",
    "        speaker=speaker,\n",
    "        pace=1.0,\n",
    "        spec=mels,\n",
    "        attn_prior=attn_prior,\n",
    "        mel_lens=spec_len,\n",
    "        input_lens=text_lens,\n",
    "    )\n",
    "\n",
    "    if durs is None:\n",
    "        durs = attn_hard_dur\n",
    "\n",
    "    mel_loss = spec_gen.mel_loss(spect_predicted=mels_pred, spect_tgt=mels)\n",
    "    dur_loss = spec_gen.duration_loss(\n",
    "        log_durs_predicted=log_durs_pred.cuda(), durs_tgt=durs.cuda(), len=text_lens.cuda())\n",
    "    spec_gen_loss = mel_loss + dur_loss\n",
    "\n",
    "    if pitch is not None:    \n",
    "        pitch_loss = spec_gen.pitch_loss(pitch_predicted=pitch_pred.cuda(), pitch_tgt=pitch,\n",
    "                                      len=text_lens.cuda())\n",
    "    else:\n",
    "        pitch_loss = 0\n",
    "    spec_gen_loss += pitch_loss\n",
    "\n",
    "    spec_gen.on_before_zero_grad(spec_gen._optimizer)\n",
    "    spec_gen.optimizer_zero_grad(epoch, batch_idx, spec_gen._optimizer, batch_idx)\n",
    "\n",
    "    spec_gen.on_before_backward(spec_gen_loss)\n",
    "    spec_gen.backward(spec_gen_loss, spec_gen._optimizer, batch_idx)\n",
    "    spec_gen.on_after_backward()\n",
    "\n",
    "    spec_gen.on_before_optimizer_step(spec_gen._optimizer, batch_idx)\n",
    "    spec_gen.optimizer_step(epoch, batch_idx, spec_gen._optimizer, batch_idx)\n",
    "\n",
    "    spec_gen.on_train_batch_end(mels_pred, tts_batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    # vocoder\n",
    "\n",
    "    vocoder.on_train_batch_start(tts_batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    vocoder.on_before_batch_transfer(tts_batch, dataloader_idx)\n",
    "    vocoder.transfer_batch_to_device(tts_batch, torch.device(\"cuda\"), dataloader_idx)\n",
    "    vocoder.on_after_batch_transfer(tts_batch, dataloader_idx)\n",
    "    \n",
    "    audio_pred = vocoder.forward(spec=mels_pred)\n",
    "\n",
    "    vocoder_loss = 0\n",
    "\n",
    "    audio_ = audio_pred.squeeze(1)[:, :audio.shape[1]]\n",
    "\n",
    "    sc_loss, mag_loss = vocoder.loss(x=audio_, y=audio)\n",
    "    loss_feat = sum(sc_loss) + sum(mag_loss)\n",
    "    loss_feat /= len(sc_loss)\n",
    "    vocoder_loss += loss_feat\n",
    "\n",
    "    vocoder.on_before_zero_grad(vocoder._optimizer)\n",
    "    vocoder.optimizer_zero_grad(epoch, batch_idx, vocoder._optimizer, batch_idx)\n",
    "\n",
    "    vocoder.on_before_backward(vocoder_loss)\n",
    "#     try:\n",
    "#         vocoder.backward(vocoder_loss, vocoder._optimizer, batch_idx)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "    vocoder.on_after_backward()\n",
    "\n",
    "    vocoder.on_before_optimizer_step(vocoder._optimizer, batch_idx)\n",
    "    vocoder.optimizer_step(epoch, batch_idx, vocoder._optimizer, batch_idx)\n",
    "\n",
    "    vocoder.on_train_batch_end(audio_, tts_batch, batch_idx, dataloader_idx)\n",
    "    \n",
    "    loss = torch_mse_loss(audio_, audio)\n",
    "    \n",
    "    del audio_pred\n",
    "    del sc_loss, mag_loss\n",
    "    del audio, audio_\n",
    "#     del spec_gen_loss, vocoder_loss\n",
    "    del loss_feat\n",
    "        \n",
    "    return spec_gen_loss.cpu().detach(), vocoder_loss.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_transformation_asr_step(batch_idx, raw_signal_batch, dataloader_idx=0):\n",
    "    signal, signal_len, _, _ = raw_signal_batch\n",
    "    signal, signal_len = signal.cuda(), signal_len.cuda()\n",
    "\n",
    "    log_probs, encoded_len, predictions = asr_model.forward(input_signal=signal, input_signal_length=signal_len)\n",
    "\n",
    "    current_hypotheses = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "        predictions, predictions_len=encoded_len, return_hypotheses=False,\n",
    "    )\n",
    "        \n",
    "    transcript = spec_gen.parse('Hi')\n",
    "        \n",
    "    transcript_len = torch.tensor([len(transcript)])\n",
    "        \n",
    "    batch = (signal, signal_len, transcript, transcript_len, None, None, None)\n",
    "    \n",
    "    return paired_spec_gen_vocoder_step(batch_idx, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_transformation_tts_step(batch_idx, raw_text_batch, dataloader_idx=0):\n",
    "    s, s_l, text, text_lens, _, _, _ = raw_text_batch\n",
    "    \n",
    "    s, s_l, text, text_lens = s.cuda(), s_l.cuda(), text.cuda(), text_lens.cuda()\n",
    "    \n",
    "#     target = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "#         text, predictions_len=text_lens, return_hypotheses=False,\n",
    "#     )\n",
    "        \n",
    "#     parsed = spec_gen.parse(target[0])\n",
    "          \n",
    "#     print(text)\n",
    "        \n",
    "    spec, _, durs_pred, _, pitch_pred, *_ = spec_gen.forward(text=text,\n",
    "                                                        durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "        \n",
    "#     print(spec, spec.shape)\n",
    "        \n",
    "    signal = vocoder.forward(spec=spec)\n",
    "    signal_len = torch.tensor([len(signal[0])])\n",
    "    \n",
    "    signal = signal.squeeze(0)\n",
    "    \n",
    "    print('s', s.shape)\n",
    "    print('s', signal.shape)\n",
    "        \n",
    "    return paired_asr_training_step(batch_idx, (signal, signal_len, text, text_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end():\n",
    "    asr_model.on_train_epoch_end() \n",
    "    spec_gen.on_train_epoch_end()\n",
    "    vocoder.on_train_epoch_end()\n",
    "    \n",
    "    asr_model.on_epoch_end()\n",
    "    spec_gen.on_epoch_end()\n",
    "    vocoder.on_epoch_end()\n",
    "    \n",
    "def on_train_end():\n",
    "    asr_model.on_train_end()\n",
    "    spec_gen.on_train_end()\n",
    "    vocoder.on_train_end()\n",
    "\n",
    "    asr_model.on_fit_end()\n",
    "    spec_gen.on_fit_end()\n",
    "    vocoder.on_fit_end()\n",
    "\n",
    "    asr_model.teardown(\"fit\")\n",
    "    spec_gen.teardown(\"fit\")\n",
    "    vocoder.teardown(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_batch(batch):\n",
    "    for i in range(len(batch)):\n",
    "        try:\n",
    "            for j in range(len(batch[i])):\n",
    "                batch[i][j] = torch.flip(batch[i][j], dims=[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-01-21 16:44:52 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f95fa547990>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 300\n",
    "\n",
    "dataloader_idx = 0\n",
    "\n",
    "on_train_start()\n",
    "\n",
    "overall_loss = {'epoch': [], 'asr_model_loss': [], 'spec_gen_loss': [], 'vocoder_loss': []}\n",
    "\n",
    "torch_mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_loss = {'asr_model_loss': [], 'spec_gen_loss': [], 'vocoder_loss': []}\n",
    "    \n",
    "    on_epoch_start()\n",
    "    \n",
    "    for batch_idx, (asr_batch, tts_batch, raw_signal_batch, raw_text_batch) in \\\n",
    "        enumerate(zip(asr_model.train_dataloader(), spec_gen.train_dataloader(), \n",
    "                     asr_model.val_dataloader(), spec_gen.val_dataloader())):\n",
    "        \n",
    "#         asr_model_loss = dual_transformation_tts_step(batch_idx, raw_text_batch)\n",
    "\n",
    "\n",
    "        asr_model_loss = paired_asr_training_step(batch_idx, asr_batch) \n",
    "        \n",
    "        asr_model_loss += paired_asr_training_step(batch_idx, flip_batch(asr_batch)) \n",
    "        \n",
    "        epoch_loss['asr_model_loss'].append(asr_model_loss)\n",
    "        \n",
    "        spec_gen_loss, vocoder_loss = paired_spec_gen_vocoder_step(batch_idx, tts_batch)\n",
    "        _sgl, _vl = paired_spec_gen_vocoder_step(batch_idx, flip_batch(tts_batch))\n",
    "        spec_gen_loss += _sgl\n",
    "        vocoder_loss += _vl\n",
    "        \n",
    "        _sgl, _vl = dual_transformation_asr_step(batch_idx, raw_signal_batch)\n",
    "        spec_gen_loss += _sgl\n",
    "        vocoder_loss += _vl\n",
    "        _sgl, _vl = dual_transformation_asr_step(batch_idx, flip_batch(raw_signal_batch))\n",
    "        spec_gen_loss += _sgl\n",
    "        vocoder_loss += _vl\n",
    "                \n",
    "        epoch_loss['spec_gen_loss'].append(spec_gen_loss)\n",
    "        epoch_loss['vocoder_loss'].append(vocoder_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(asr_model_loss, spec_gen_loss, vocoder_loss)\n",
    "        \n",
    "#         del asr_model_loss, spec_gen_loss, vocoder_loss\n",
    "        \n",
    "    overall_loss['epoch'].append(epoch)\n",
    "    overall_loss['asr_model_loss'].append(torch.mean(torch.tensor(epoch_loss['asr_model_loss'])))\n",
    "    overall_loss['spec_gen_loss'].append(torch.mean(torch.tensor(epoch_loss['spec_gen_loss'])))\n",
    "    overall_loss['vocoder_loss'].append(torch.mean(torch.tensor(epoch_loss['vocoder_loss'])))\n",
    "\n",
    "    \n",
    "    \n",
    "    print(overall_loss)\n",
    "\n",
    "    on_epoch_end()\n",
    "    \n",
    "on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, batch in enumerate(UnpairedAudioToBPEDataset(manifest_filepath=\"../../datasets/LJSpeech-1.1/small_manifest.json\",\n",
    "#                     unpaired_text_manifest_filepath=\"../../datasets/LJSpeech-1.1/train_manifest_text.json\",\n",
    "#                     unpaired_signal_manifest_filepath=\"../../datasets/LJSpeech-1.1/train_manifest_signal.json\",                   \n",
    "#                     tokenizer=asr_model.tokenizer,\n",
    "#                     sample_rate=22050)):    \n",
    "    \n",
    "#     signal, signal_len, transcript, transcript_len, raw_signal, raw_signal_len, raw_text, raw_text_len = batch\n",
    "# #     signal, signal_len, transcript, transcript_len = signal.cuda(), signal_len.cuda(), \\\n",
    "# #         transcript.cuda(), transcript_len.cuda()\n",
    "#     print(raw_text)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    torch.set_grad_enabled(False)\n",
    "    asr_model.on_validation_start()\n",
    "    asr_model.on_epoch_start()\n",
    "    asr_model.on_validation_epoch_start()\n",
    "\n",
    "    val_outs = []\n",
    "\n",
    "    for batch_idx, val_batch in enumerate(asr_model.val_dataloader()):\n",
    "\n",
    "        asr_model.on_validation_batch_start(val_batch, batch_idx, dataloader_idx)\n",
    "\n",
    "        batch = asr_model.on_before_batch_transfer(val_batch, dataloader_idx)\n",
    "        batch = asr_model.transfer_batch_to_device(batch, torch.device('cuda'), dataloader_idx)\n",
    "        batch = asr_model.on_after_batch_transfer(batch, dataloader_idx)\n",
    "\n",
    "        out = asr_model.validation_step(batch, batch_idx)\n",
    "\n",
    "        val_loss, val_wer_num, val_wer_denom, val_wer = out\n",
    "        \n",
    "        asr_model.on_validation_batch_end(out, batch, batch_idx, dataloader_idx)\n",
    "        val_outs.append(out)\n",
    "        \n",
    "    asr_model.on_epoch_end()\n",
    "    asr_model.on_validation_end()\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    return val_outs\n",
    "\n",
    "val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74e10b9e14d4d7fbdded983311644da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['⁇ ss ⁇', '⁇ ss ⁇', '⁇ ss ⁇']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.transcribe([\"../../datasets//LJSpeech-1.1/wavs/LJ015-0247.wav\",\n",
    "                     \"../../datasets//LJSpeech-1.1/wavs/LJ028-0475.wav\",\n",
    "                     \"../../datasets//LJSpeech-1.1/wavs/LJ013-0049.wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def fastpitch_training_step(self, audio, audio_lens, text, text_lens, durs, pitch, batch_idx=1):\n",
    "#     mels, spec_len = self.preprocessor(input_signal=audio, length=audio_lens)\n",
    "\n",
    "#     mels_pred, _, _, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = self(\n",
    "#         text=text,\n",
    "#         durs=durs,\n",
    "#         pitch=pitch,\n",
    "#         speaker=None,\n",
    "#         pace=1.0,\n",
    "#         spec=mels if self.learn_alignment else None,\n",
    "#         attn_prior=None,\n",
    "#         mel_lens=spec_len,\n",
    "#         input_lens=text_lens,\n",
    "#     )\n",
    "#     if durs is None:\n",
    "#         durs = attn_hard_dur\n",
    "\n",
    "#     mel_loss = self.mel_loss(spect_predicted=mels_pred, spect_tgt=mels)\n",
    "#     dur_loss = self.duration_loss(log_durs_predicted=log_durs_pred, durs_tgt=durs, len=text_lens)\n",
    "#     loss = mel_loss + dur_loss\n",
    "\n",
    "#     pitch_loss = self.pitch_loss(pitch_predicted=pitch_pred, pitch_tgt=pitch, len=text_lens)\n",
    "#     loss += pitch_loss\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# def hifigan_training_step(self, batch, batch_idx, optimizer_idx):\n",
    "#     # if in finetune mode the mels are pre-computed using a\n",
    "#     # spectrogram generator\n",
    "#     if self.input_as_mel:\n",
    "#         audio, audio_len, audio_mel = batch\n",
    "#     # else, we compute the mel using the ground truth audio\n",
    "#     else:\n",
    "#         audio, audio_len = batch\n",
    "#         # mel as input for generator\n",
    "#         audio_mel, _ = self.audio_to_melspec_precessor(audio, audio_len)\n",
    "\n",
    "#     # mel as input for L1 mel loss\n",
    "#     audio_trg_mel, _ = self.trg_melspec_fn(audio, audio_len)\n",
    "#     audio = audio.unsqueeze(1)\n",
    "\n",
    "#     audio_pred = self.generator(x=audio_mel)\n",
    "#     audio_pred_mel, _ = self.trg_melspec_fn(audio_pred.squeeze(1), audio_len)\n",
    "\n",
    "#     # train discriminator\n",
    "# #     self.optim_d.zero_grad()\n",
    "#     mpd_score_real, mpd_score_gen, _, _ = self.mpd(y=audio, y_hat=audio_pred.detach())\n",
    "#     loss_disc_mpd, _, _ = self.discriminator_loss(\n",
    "#         disc_real_outputs=mpd_score_real, disc_generated_outputs=mpd_score_gen\n",
    "#     )\n",
    "#     msd_score_real, msd_score_gen, _, _ = self.msd(y=audio, y_hat=audio_pred.detach())\n",
    "#     loss_disc_msd, _, _ = self.discriminator_loss(\n",
    "#         disc_real_outputs=msd_score_real, disc_generated_outputs=msd_score_gen\n",
    "#     )\n",
    "#     loss_d = loss_disc_msd + loss_disc_mpd\n",
    "    \n",
    "#     return loss_d\n",
    "# #     self.manual_backward(loss_d)\n",
    "# #     self.optim_d.step()\n",
    "\n",
    "# #     # train generator\n",
    "# #     self.optim_g.zero_grad()\n",
    "# #     loss_mel = F.l1_loss(audio_pred_mel, audio_trg_mel)\n",
    "# #     _, mpd_score_gen, fmap_mpd_real, fmap_mpd_gen = self.mpd(y=audio, y_hat=audio_pred)\n",
    "# #     _, msd_score_gen, fmap_msd_real, fmap_msd_gen = self.msd(y=audio, y_hat=audio_pred)\n",
    "# #     loss_fm_mpd = self.feature_loss(fmap_r=fmap_mpd_real, fmap_g=fmap_mpd_gen)\n",
    "# #     loss_fm_msd = self.feature_loss(fmap_r=fmap_msd_real, fmap_g=fmap_msd_gen)\n",
    "# #     loss_gen_mpd, _ = self.generator_loss(disc_outputs=mpd_score_gen)\n",
    "# #     loss_gen_msd, _ = self.generator_loss(disc_outputs=msd_score_gen)\n",
    "# #     loss_g = loss_gen_msd + loss_gen_mpd + loss_fm_msd + loss_fm_mpd + loss_mel * self.l1_factor\n",
    "# #     self.manual_backward(loss_g)\n",
    "# #     self.optim_g.step()\n",
    "\n",
    "# def citrinet_training_step(self, batch, batch_nb):\n",
    "#     signal, signal_len, transcript, transcript_len = batch\n",
    "    \n",
    "#     log_probs, encoded_len, predictions = self.forward(input_signal=signal, input_signal_length=signal_len)\n",
    "\n",
    "#     loss_value = self.loss(\n",
    "#         log_probs=log_probs, targets=transcript, input_lengths=encoded_len, target_lengths=transcript_len\n",
    "#     )\n",
    "\n",
    "#     return loss_value\n",
    "\n",
    "# # for batch in asr_model.test_dataloader():\n",
    "# #     signal, signal_len, transcript, transcript_len = batch\n",
    "        \n",
    "# #     target = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "# #         transcript, predictions_len=transcript_len, return_hypotheses=False,\n",
    "# #     )\n",
    "    \n",
    "# #     parsed = fastpitch.parse(target[0])\n",
    "    \n",
    "# #     spec, _, durs_pred, _, pitch_pred, *_ = fastpitch.eval()(text=parsed,\n",
    "# #                                                              durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "        \n",
    "# #     loss = fastpitch_training_step(fastpitch.cpu().train(), signal, signal_len, parsed,\n",
    "# #                                    torch.tensor([len(parsed)]), durs_pred, pitch_pred, batch_idx=1).cpu()\n",
    "    \n",
    "# #     hifigan.input_as_mel = True\n",
    "# #     loss_2 = hifigan_training_step(hifigan.cpu().train(), batch=[signal, signal_len, spec], batch_idx=1, optimizer_idx=1)\n",
    "    \n",
    "# #     loss_asr = citrinet_training_step(asr_model.cpu().train(), batch=batch, batch_nb=1)\n",
    "    \n",
    "# #     print(loss, loss_2, loss_asr)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dual Transformation: text -> predicted_speech -> predicted_text -> loss\n",
    "# def dt_text_training_step(batch, batch_idx, optimizer_idx=None):\n",
    "#     if optimizer_idx is None:\n",
    "#         optimizer_idx = batch_idx\n",
    "    \n",
    "#     _, _, transcript, transcript_len = batch\n",
    "        \n",
    "#     target = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "#         transcript, predictions_len=transcript_len, return_hypotheses=False,\n",
    "#     )\n",
    "    \n",
    "#     parsed = fastpitch.parse(target[0]).cpu()\n",
    "    \n",
    "#     spec, _, durs_pred, _, pitch_pred, *_ = fastpitch.cpu().eval()(text=parsed,\n",
    "#                                                              durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "        \n",
    "#     fastpitch.eval()(text=parsed, durs=durs_pred, pitch=pitch_pred, speaker=None, pace=1.0)\n",
    "    \n",
    "#     signal = hifigan.cpu().eval().convert_spectrogram_to_audio(spec=spec).to('cpu')\n",
    "#     signal_len = torch.tensor([len(signal[0])])\n",
    "        \n",
    "#     loss = citrinet_training_step(asr_model.cpu().train(),\n",
    "#                                   batch=[signal, signal_len, transcript, transcript_len], batch_nb=batch_idx)\n",
    "\n",
    "#     fastpitch_training_step(fastpitch.cpu().train(), signal, signal_len, parsed,\n",
    "#                                    torch.tensor([len(parsed)]), durs_pred, pitch_pred, batch_idx=batch_idx).cpu()\n",
    "\n",
    "#     hifigan.input_as_mel = True\n",
    "#     hifigan_training_step(hifigan.cpu().train(),\n",
    "#                                    batch=[signal, signal_len, spec], batch_idx=batch_idx, optimizer_idx=optimizer_idx)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dual Transformation: speech -> predicted_text -> predicted_speech -> loss\n",
    "# def dt_speech_training_step(batch, batch_idx, optimizer_idx=None):\n",
    "#     if optimizer_idx is None:\n",
    "#         optimizer_idx = batch_idx\n",
    "        \n",
    "#     signal, signal_len, _, _ = batch\n",
    "\n",
    "#     logits, logits_len, greedy_predictions = asr_model.eval().cpu()(input_signal=signal,\n",
    "#                                                                     input_signal_length=signal_len)\n",
    "#     current_hypotheses = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "#         greedy_predictions, predictions_len=logits_len\n",
    "#     )\n",
    "    \n",
    "#     transcript = fastpitch.parse(current_hypotheses[0])\n",
    "#     transcript_len = torch.tensor([len(transcript[0])]) \n",
    "        \n",
    "#     spec, _, durs_pred, _, pitch_pred, *_ = fastpitch.eval()(text=transcript, durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "#     gt = train_batch[0]\n",
    "# #     pred = hifigan.cpu().eval().convert_spectrogram_to_audio(spec=spec.cpu()).to('cpu')\n",
    "                 \n",
    "# #     citrinet_training_step(asr_model.cpu().train(),\n",
    "# #                                   batch=[signal, signal_len, transcript, transcript_len], batch_nb=batch_idx)\n",
    "\n",
    "# #     fastpitch_training_step(fastpitch.cpu().train(), signal, signal_len, transcript,\n",
    "# #                                    transcript_len.cpu(), durs_pred, pitch_pred, batch_idx=batch_idx).cpu()\n",
    "\n",
    "# #     hifigan.input_as_mel = True\n",
    "# #     hifigan_training_step(hifigan.cpu().train(),\n",
    "# #                                    batch=[signal, signal_len, spec], batch_idx=batch_idx, optimizer_idx=optimizer_idx)    \n",
    "    \n",
    "# #     return ((signal[0][:min(len(signal[0]), len(pred[0]))] - \n",
    "# #              pred[0][:min(len(signal[0]), len(pred[0]))].detach().numpy()) ** 2).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step, train_batch in enumerate(asr_model.train_dataloader()):\n",
    "#     loss = 0\n",
    "    \n",
    "#     loss += dt_text_training_step(train_batch, step)\n",
    "# #     loss += dt_text_training_step(flip_batch(train_batch), step)\n",
    "    \n",
    "# #     loss += dt_speech_training_step(train_batch, step)\n",
    "# #     loss += dt_text_training_step(flip_batch(train_batch), step)\n",
    "    \n",
    "#     ###\n",
    "#     # Add loss for paired data\n",
    "#     ###\n",
    "\n",
    "#     ###\n",
    "#     # Add loss for Denoising Auto-Encoder\n",
    "#     ###\n",
    "\n",
    "#     # print(step, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
