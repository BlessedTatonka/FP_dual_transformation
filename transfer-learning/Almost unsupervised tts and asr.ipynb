{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-12-20 05:32:28 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-12-20 05:32:28 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2021-12-20 05:32:28 nmse_clustering:54] Using eigen decomposition from scipy, upgrade torch to 1.9 or higher for faster clustering\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-12-20 05:32:28 nemo_logging:349] /home/boris/anaconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      '\"sox\" backend is being deprecated. '\n",
      "    \n",
      "[NeMo W 2021-12-20 05:32:28 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import DictConfig\n",
    "import pathlib\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from math import ceil\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import ChainDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nemo.collections.asr.data import audio_to_text_dataset\n",
    "from nemo.collections.asr.data.audio_to_text_dali import DALIOutputs\n",
    "from nemo.collections.asr.losses.ctc import CTCLoss\n",
    "from nemo.collections.asr.metrics.wer import WER\n",
    "from nemo.collections.asr.models.asr_model import ASRModel, ExportableEncDecModel\n",
    "from nemo.collections.asr.parts.mixins import ASRModuleMixin\n",
    "from nemo.collections.asr.parts.preprocessing.perturb import process_augmentations\n",
    "from nemo.core.classes.common import PretrainedModelInfo, typecheck\n",
    "from nemo.core.neural_types import AudioSignal, LabelsType, LengthsType, LogprobsType, NeuralType, SpectrogramType\n",
    "from nemo.utils import logging\n",
    "from nemo.collections.tts.models import FastPitchHifiGanE2EModel\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Reduce logging messages for this notebook\n",
    "from nemo.utils import logging\n",
    "logging.setLevel(logging.ERROR)\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.helpers.helpers import regulate_len\n",
    "\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from ruamel.yaml import YAML\n",
    "except ModuleNotFoundError:\n",
    "    from ruamel_yaml import YAML\n",
    "\n",
    "config_path = 'stt_en_citrinet_256_gamma_0_25'\n",
    "config_name = 'model_config.yaml'\n",
    "yaml = YAML(typ='safe')\n",
    "\n",
    "with open(os.path.join(config_path, config_name)) as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "config['tokenizer']['dir'] = 'citrinet_tokenizer/tokenizer_spe_unigram_v1024'\n",
    "config['tokenizer']['type'] = 'bpe'\n",
    "\n",
    "# config['train_ds']['manifest_filepath']=\"../../datasets/LJSpeech-1.1/train_manifest.json\"\n",
    "config['train_ds']['manifest_filepath']=\"../../datasets/an4/train_manifest.json\"\n",
    "config['train_ds']['batch_size'] = 1\n",
    "config['train_ds']['num_workers'] = 12\n",
    "config['train_ds']['pin_memory'] = True\n",
    "\n",
    "# config['validation_ds']['manifest_filepath']=\"../../datasets/LJSpeech-1.1/test_manifest.json\"\n",
    "config['validation_ds']['manifest_filepath']=\"../../datasets/an4/test_manifest.json\"\n",
    "config['validation_ds']['batch_size'] = 1\n",
    "config['validation_ds']['num_workers'] = 12\n",
    "config['validation_ds']['pin_memory'] = True\n",
    "\n",
    "config['spec_augment']['freq_masks'] = 0\n",
    "config['spec_augment']['time_masks'] = 0\n",
    "config['optim']['lr'] = 0.01\n",
    "config['optim']['name'] = 'novograd'\n",
    "config['optim']['betas'] = [0.8, 0.25]\n",
    "config['optim']['weight_decay'] = 0.001\n",
    "config['optim']['sched']['warmup_steps']=1000\n",
    "config['optim']['sched']['min_lr'] = 0.00001\n",
    "\n",
    "config['tokenizer']['model_path'] = 'stt_en_citrinet_256_gamma_0_25/3d20ebb793c84a64a20c7ad26fc64d62_tokenizer.model'\n",
    "config['tokenizer']['vocab_path'] = 'stt_en_citrinet_256_gamma_0_25/df5191f216004f10a268c44e90fdb63f_vocab.txt'\n",
    "config['tokenizer']['spe_tokenizer_vocab'] = 'stt_en_citrinet_256_gamma_0_25/b774eaac83804907843607272fde21a4_tokenizer.vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr_model = nemo_asr.models.EncDecCTCModelBPE(cfg=DictConfig(config))\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained('stt_en_citrinet_256_gamma_0_25') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastpitch = FastPitchModel.from_pretrained(\"tts_en_fastpitch\")\n",
    "hifigan = HifiGanModel.from_pretrained(\"tts_hifigan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch.cfg['train_ds']['sample_rate'] = 16000\n",
    "fastpitch.cfg['preprocessor']['sample_rate'] = 16000\n",
    "fastpitch.cfg['validation_ds']['sample_rate'] = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan.cfg['preprocessor']['sample_rate'] = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.setup_training_data(train_data_config=config['train_ds'])\n",
    "asr_model.setup_test_data(test_data_config=config['validation_ds'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fastpitch_training_step(self, audio, audio_lens, text, text_lens, durs, pitch, batch_idx=1):\n",
    "    mels, spec_len = self.preprocessor(input_signal=audio, length=audio_lens)\n",
    "\n",
    "    mels_pred, _, _, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = self(\n",
    "        text=text,\n",
    "        durs=durs,\n",
    "        pitch=pitch,\n",
    "        speaker=None,\n",
    "        pace=1.0,\n",
    "        spec=mels if self.learn_alignment else None,\n",
    "        attn_prior=None,\n",
    "        mel_lens=spec_len,\n",
    "        input_lens=text_lens,\n",
    "    )\n",
    "    if durs is None:\n",
    "        durs = attn_hard_dur\n",
    "\n",
    "    mel_loss = self.mel_loss(spect_predicted=mels_pred, spect_tgt=mels)\n",
    "    dur_loss = self.duration_loss(log_durs_predicted=log_durs_pred, durs_tgt=durs, len=text_lens)\n",
    "    loss = mel_loss + dur_loss\n",
    "\n",
    "    pitch_loss = self.pitch_loss(pitch_predicted=pitch_pred, pitch_tgt=pitch, len=text_lens)\n",
    "    loss += pitch_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "def hifigan_training_step(self, batch, batch_idx, optimizer_idx):\n",
    "    # if in finetune mode the mels are pre-computed using a\n",
    "    # spectrogram generator\n",
    "    if self.input_as_mel:\n",
    "        audio, audio_len, audio_mel = batch\n",
    "    # else, we compute the mel using the ground truth audio\n",
    "    else:\n",
    "        audio, audio_len = batch\n",
    "        # mel as input for generator\n",
    "        audio_mel, _ = self.audio_to_melspec_precessor(audio, audio_len)\n",
    "\n",
    "    # mel as input for L1 mel loss\n",
    "    audio_trg_mel, _ = self.trg_melspec_fn(audio, audio_len)\n",
    "    audio = audio.unsqueeze(1)\n",
    "\n",
    "    audio_pred = self.generator(x=audio_mel)\n",
    "    audio_pred_mel, _ = self.trg_melspec_fn(audio_pred.squeeze(1), audio_len)\n",
    "\n",
    "    # train discriminator\n",
    "#     self.optim_d.zero_grad()\n",
    "    mpd_score_real, mpd_score_gen, _, _ = self.mpd(y=audio, y_hat=audio_pred.detach())\n",
    "    loss_disc_mpd, _, _ = self.discriminator_loss(\n",
    "        disc_real_outputs=mpd_score_real, disc_generated_outputs=mpd_score_gen\n",
    "    )\n",
    "    msd_score_real, msd_score_gen, _, _ = self.msd(y=audio, y_hat=audio_pred.detach())\n",
    "    loss_disc_msd, _, _ = self.discriminator_loss(\n",
    "        disc_real_outputs=msd_score_real, disc_generated_outputs=msd_score_gen\n",
    "    )\n",
    "    loss_d = loss_disc_msd + loss_disc_mpd\n",
    "    \n",
    "    return loss_d\n",
    "#     self.manual_backward(loss_d)\n",
    "#     self.optim_d.step()\n",
    "\n",
    "#     # train generator\n",
    "#     self.optim_g.zero_grad()\n",
    "#     loss_mel = F.l1_loss(audio_pred_mel, audio_trg_mel)\n",
    "#     _, mpd_score_gen, fmap_mpd_real, fmap_mpd_gen = self.mpd(y=audio, y_hat=audio_pred)\n",
    "#     _, msd_score_gen, fmap_msd_real, fmap_msd_gen = self.msd(y=audio, y_hat=audio_pred)\n",
    "#     loss_fm_mpd = self.feature_loss(fmap_r=fmap_mpd_real, fmap_g=fmap_mpd_gen)\n",
    "#     loss_fm_msd = self.feature_loss(fmap_r=fmap_msd_real, fmap_g=fmap_msd_gen)\n",
    "#     loss_gen_mpd, _ = self.generator_loss(disc_outputs=mpd_score_gen)\n",
    "#     loss_gen_msd, _ = self.generator_loss(disc_outputs=msd_score_gen)\n",
    "#     loss_g = loss_gen_msd + loss_gen_mpd + loss_fm_msd + loss_fm_mpd + loss_mel * self.l1_factor\n",
    "#     self.manual_backward(loss_g)\n",
    "#     self.optim_g.step()\n",
    "\n",
    "def citrinet_training_step(self, batch, batch_nb):\n",
    "    signal, signal_len, transcript, transcript_len = batch\n",
    "    \n",
    "    log_probs, encoded_len, predictions = self.forward(input_signal=signal, input_signal_length=signal_len)\n",
    "\n",
    "    loss_value = self.loss(\n",
    "        log_probs=log_probs, targets=transcript, input_lengths=encoded_len, target_lengths=transcript_len\n",
    "    )\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "# for batch in asr_model.test_dataloader():\n",
    "#     signal, signal_len, transcript, transcript_len = batch\n",
    "        \n",
    "#     target = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "#         transcript, predictions_len=transcript_len, return_hypotheses=False,\n",
    "#     )\n",
    "    \n",
    "#     parsed = fastpitch.parse(target[0])\n",
    "    \n",
    "#     spec, _, durs_pred, _, pitch_pred, *_ = fastpitch.eval()(text=parsed,\n",
    "#                                                              durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "        \n",
    "#     loss = fastpitch_training_step(fastpitch.cpu().train(), signal, signal_len, parsed,\n",
    "#                                    torch.tensor([len(parsed)]), durs_pred, pitch_pred, batch_idx=1).cpu()\n",
    "    \n",
    "#     hifigan.input_as_mel = True\n",
    "#     loss_2 = hifigan_training_step(hifigan.cpu().train(), batch=[signal, signal_len, spec], batch_idx=1, optimizer_idx=1)\n",
    "    \n",
    "#     loss_asr = citrinet_training_step(asr_model.cpu().train(), batch=batch, batch_nb=1)\n",
    "    \n",
    "#     print(loss, loss_2, loss_asr)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual Transformation: text -> predicted_speech -> predicted_text -> loss\n",
    "def dt_text_training_step(batch, batch_idx, optimizer_idx=None):\n",
    "    if optimizer_idx is None:\n",
    "        optimizer_idx = batch_idx\n",
    "    \n",
    "    _, _, transcript, transcript_len = batch\n",
    "        \n",
    "    target = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "        transcript, predictions_len=transcript_len, return_hypotheses=False,\n",
    "    )\n",
    "    \n",
    "    parsed = fastpitch.parse(target[0]).cpu()\n",
    "    \n",
    "    spec, _, durs_pred, _, pitch_pred, *_ = fastpitch.cpu().eval()(text=parsed,\n",
    "                                                             durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "        \n",
    "    fastpitch.eval()(text=parsed, durs=durs_pred, pitch=pitch_pred, speaker=None, pace=1.0)\n",
    "    \n",
    "    signal = hifigan.cpu().eval().convert_spectrogram_to_audio(spec=spec).to('cpu')\n",
    "    signal_len = torch.tensor([len(signal[0])])\n",
    "        \n",
    "    loss = citrinet_training_step(asr_model.cpu().train(),\n",
    "                                  batch=[signal, signal_len, transcript, transcript_len], batch_nb=batch_idx)\n",
    "\n",
    "    fastpitch_training_step(fastpitch.cpu().train(), signal, signal_len, parsed,\n",
    "                                   torch.tensor([len(parsed)]), durs_pred, pitch_pred, batch_idx=batch_idx).cpu()\n",
    "\n",
    "    hifigan.input_as_mel = True\n",
    "    hifigan_training_step(hifigan.cpu().train(),\n",
    "                                   batch=[signal, signal_len, spec], batch_idx=batch_idx, optimizer_idx=optimizer_idx)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.8404, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dual Transformation: speech -> predicted_text -> predicted_speech -> loss\n",
    "def dt_speech_training_step(batch, batch_idx, optimizer_idx=None):\n",
    "    if optimizer_idx is None:\n",
    "        optimizer_idx = batch_idx\n",
    "        \n",
    "    signal, signal_len, _, _ = batch\n",
    "\n",
    "    logits, logits_len, greedy_predictions = asr_model.eval().cpu()(input_signal=signal,\n",
    "                                                                    input_signal_length=signal_len)\n",
    "    current_hypotheses = asr_model._wer.ctc_decoder_predictions_tensor(\n",
    "        greedy_predictions, predictions_len=logits_len\n",
    "    )\n",
    "    \n",
    "    transcript = fastpitch.parse(current_hypotheses[0])\n",
    "    transcript_len = torch.tensor([len(transcript[0])]) \n",
    "        \n",
    "    spec, _, durs_pred, _, pitch_pred, *_ = fastpitch.eval()(text=transcript, durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "    gt = train_batch[0]\n",
    "    pred = hifigan.cpu().eval().convert_spectrogram_to_audio(spec=spec.cpu()).to('cpu')\n",
    "                 \n",
    "    citrinet_training_step(asr_model.cpu().train(),\n",
    "                                  batch=[signal, signal_len, transcript, transcript_len], batch_nb=batch_idx)\n",
    "\n",
    "    fastpitch_training_step(fastpitch.cpu().train(), signal, signal_len, transcript,\n",
    "                                   transcript_len.cpu(), durs_pred, pitch_pred, batch_idx=batch_idx).cpu()\n",
    "\n",
    "    hifigan.input_as_mel = True\n",
    "    hifigan_training_step(hifigan.cpu().train(),\n",
    "                                   batch=[signal, signal_len, spec], batch_idx=batch_idx, optimizer_idx=optimizer_idx)    \n",
    "    \n",
    "    return ((signal[0][:min(len(signal[0]), len(pred[0]))] - \n",
    "             pred[0][:min(len(signal[0]), len(pred[0]))].detach().numpy()) ** 2).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_batch(batch):\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            batch[i][j] = torch.flip(batch[i][j], dims=[0])\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, train_batch in enumerate(asr_model.train_dataloader()):\n",
    "    loss = 0\n",
    "    \n",
    "    loss += dt_text_training_step(train_batch, step)\n",
    "    loss += dt_text_training_step(flip_batch(train_batch), step)\n",
    "    \n",
    "    loss += dt_speech_training_step(train_batch, step)\n",
    "    loss += dt_text_training_step(flip_batch(train_batch), step)\n",
    "    \n",
    "    ###\n",
    "    # Add loss for paired data\n",
    "    ###\n",
    "\n",
    "    ###\n",
    "    # Add loss for Denoising Auto-Encoder\n",
    "    ###\n",
    "\n",
    "    # print(step, loss)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
