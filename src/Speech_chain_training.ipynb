{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984bc0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/boris/F/anaconda3/envs/nemo/lib/python3.9/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=media/boris/F/pykaldi/kaldi\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-05-26 18:31:48 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "import wget\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "import json\n",
    "import librosa\n",
    "\n",
    "import librosa\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "import copy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from IPython.utils import io\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import zip_longest\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.utilities import rank_zero_info\n",
    "\n",
    "import json\n",
    "from nemo.collections.tts.models import MelGanModel\n",
    "from nemo.collections.tts.models import MixerTTSModel\n",
    "import soundfile as sf\n",
    "\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ae93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "try:\n",
    "    from ruamel.yaml import YAML\n",
    "except ModuleNotFoundError:\n",
    "    from ruamel_yaml import YAML\n",
    "\n",
    "yaml = YAML(typ='safe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47575ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from parts.models import EncDecCTCModelBPE_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc49b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/media/boris/F/Research_final/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7e13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EncDecCTCModelBPE_extended.from_config_dict(DictConfig(asr_cfg['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5cdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_batch(batch):\n",
    "    new_batch = torch.clone(batch[0]), torch.clone(batch[1]), torch.clone(batch[2]), \\\n",
    "        torch.clone(batch[3])\n",
    "    for i in range(len(new_batch)):\n",
    "        try:\n",
    "            for j in range(len(new_batch[i])):\n",
    "                new_batch[i][j] = torch.flip(new_batch[i][j], dims=[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a36bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerTTSModel_extended(MixerTTSModel):\n",
    "    def __init__(self, cfg: DictConfig, trainer=None, corruption_prob=0, use_dae=False, use_bsm=False):\n",
    "        super().__init__(cfg=cfg, trainer=trainer)\n",
    "        self.corruption_prob = corruption_prob\n",
    "        \n",
    "        self.use_dae = use_dae\n",
    "        self.use_bsm = use_bsm\n",
    "    \n",
    "    \n",
    "    def forward(self, text, text_len, pitch=None, spect=None, spect_len=None, attn_prior=None, lm_tokens=None):\n",
    "        \n",
    "        if self.use_dae:\n",
    "            for i in range(text.shape[0]):\n",
    "                if i // 2:\n",
    "                    for j in range(text.shape[1]):\n",
    "                        if random.random() < self.corruption_prob:\n",
    "                            text[i][j] = random.randint(0, 339)\n",
    "        \n",
    "        return super().forward(text=text,\n",
    "                        text_len=text_len,\n",
    "                        pitch=pitch,\n",
    "                        spect=spect,\n",
    "                        spect_len=spect_len,\n",
    "                        attn_prior=attn_prior,\n",
    "                        lm_tokens=lm_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac73d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechChainModel(nemo_asr.models.EncDecCTCModelBPE):\n",
    "    def __init__(self, cfg: DictConfig, trainer=None, corruption_prob=0):\n",
    "        super().__init__(cfg=cfg, trainer=trainer)\n",
    "        \n",
    "        self.use_dae = cfg['speech_chain'].get('use_dae', False)\n",
    "        self.corruption_prob = cfg['speech_chain'].get('corruption_prob', 0)\n",
    "        self.use_bsm = cfg['speech_chain'].get('use_bsm', False)\n",
    "        \n",
    "        spec_gen_config_path = cfg['speech_chain'].get('spec_gen_config_path', None)\n",
    "        with open(spec_gen_config_path) as f:\n",
    "            spec_gen_cfg = yaml.load(f)\n",
    "        \n",
    "        self.spec_gen = MixerTTSModel.from_config_dict(DictConfig(spec_gen_cfg['model']))\n",
    "        \n",
    "#         self.spec_gen.use_dae = True\n",
    "#         self.spec_gen.use_bsm = True\n",
    "#         self.spec_gen.corruption_prob = 0.5\n",
    "        \n",
    "        self.vocoder = MelGanModel.from_pretrained(model_name=\"tts_melgan\").cuda()\n",
    "       \n",
    "        self.last_epoch_asr = 0\n",
    "        self.last_epoch_spec_gen = 0\n",
    "        self.epochs_to_pretrain = cfg['speech_chain'].get('epochs_to_pretrain', 10)\n",
    "        \n",
    "        self.paired_manifest_path = cfg['speech_chain'].get('paired_manifest_path', None)\n",
    "        self.unpaired_text_manifest_path = cfg['speech_chain'].get('unpaired_text_manifest_path', None)\n",
    "        self.unpaired_speech_manifest_path = cfg['speech_chain'].get('unpaired_speech_manifest_path', None)\n",
    "    \n",
    "        os.system('mkdir tmp')\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "    \n",
    "        ### Biderectional sequence modeling\n",
    "        if self.use_bsm:\n",
    "            flipped_batch = flip_batch(batch)\n",
    "\n",
    "            new_batch = []\n",
    "\n",
    "            new_batch.append(torch.vstack((batch[0], flipped_batch[0])))\n",
    "            new_batch.append(torch.hstack((batch[1], flipped_batch[1])))\n",
    "            new_batch.append(torch.vstack((batch[2], flipped_batch[2])))\n",
    "            new_batch.append(torch.hstack((batch[3], flipped_batch[3])))\n",
    "            batch = new_batch\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        return super().training_step(batch, batch_idx)\n",
    "    \n",
    "    def forward(\n",
    "        self, input_signal=None, input_signal_length=None, processed_signal=None, processed_signal_length=None\n",
    "    ):\n",
    "        \n",
    "        processed_signal, processed_signal_length = self.preprocessor(\n",
    "            input_signal=input_signal, length=input_signal_length,\n",
    "        )\n",
    "        \n",
    "        ### Denoising auto-encoder\n",
    "        if self.use_dae:\n",
    "            for i in range(processed_signal.shape[0]):\n",
    "                if i // 2:\n",
    "                    corruption = np.random.choice([0,1], (processed_signal[i].shape[0], processed_signal[i].shape[1]),\n",
    "                                                  p=[self.corruption_prob, 1 - self.corruption_prob])\n",
    "                    processed_signal[i] = processed_signal[i].cpu() * corruption\n",
    "                    processed_signal[i].cuda()\n",
    "\n",
    "            ###\n",
    "        \n",
    "        encoded, encoded_len = self.encoder(audio_signal=processed_signal, length=processed_signal_length)\n",
    "        log_probs = self.decoder(encoder_output=encoded)\n",
    "        greedy_predictions = log_probs.argmax(dim=-1, keepdim=False)\n",
    "\n",
    "        return log_probs, encoded_len, greedy_predictions\n",
    "    \n",
    "    def predict_audio(self):\n",
    "        os.system(f'rm ./tmp/train_manifest_asr.json')\n",
    "        os.system(f'cp {self.paired_manifest_path} ./tmp/train_manifest_asr.json')\n",
    "        \n",
    "        with open(self.unpaired_text_manifest_path) as val:\n",
    "              with open('./tmp/train_manifest_asr.json', 'a') as train:           \n",
    "                    for line in val.readlines():\n",
    "                        try:\n",
    "                            sample = json.loads(line)\n",
    "                            text = sample['text']\n",
    "\n",
    "                            parsed = self.spec_gen.parse(text)\n",
    "                            spectrogram = self.spec_gen.cuda().generate_spectrogram(tokens=parsed.cuda())\n",
    "                            audio = self.vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "                            sf.write(f\"generated_audio/{text}.wav\", audio[0].to('cpu').detach().numpy(), 22050)\n",
    "\n",
    "                            sample['audio_filepath'] = f\"generated_audio/{text}.wav\"\n",
    "                            sample['duration'] = np.round(len(audio[0]) / 22050, 1)\n",
    "\n",
    "                            train.write(str(json.dumps(sample)) + '\\n')\n",
    "\n",
    "                            del sample, text, parsed, spectrogram, audio\n",
    "                        except:\n",
    "                            continue\n",
    "        \n",
    "        d = dict(self.cfg)\n",
    "        d['train_ds']['manifest_filepath'] = './tmp/train_manifest_asr.json'\n",
    "        self.setup_training_data(DictConfig(d['train_ds']))\n",
    "    \n",
    "    def predict_text(self):\n",
    "        os.system(f'rm ./tmp/train_manifest_spec_gen.json')\n",
    "        os.system(f'cp {self.paired_manifest_path} ./tmp/train_manifest_spec_gen.json')\n",
    "        \n",
    "        with open(self.unpaired_speech_manifest_path) as val:\n",
    "              with open('./tmp/train_manifest_spec_gen.json', 'a') as train:           \n",
    "                    for i, line in enumerate(val.readlines()):\n",
    "                        sample = json.loads(line)\n",
    "                        text = self.transcribe([sample['audio_filepath']])[0].lstrip().rstrip()\n",
    "                        if text == \"\" or ('\\u2047' in text):\n",
    "                            del sample, text\n",
    "                            continue\n",
    "                            \n",
    "                        sample['text'] = text\n",
    "                    \n",
    "                        train.write(str(json.dumps(sample)) + '\\n')\n",
    "                    \n",
    "                        del sample, text\n",
    "        \n",
    "        d = dict(self.spec_gen.cfg)\n",
    "        d['train_ds']['dataset']['manifest_filepath'] = './tmp/train_manifest_spec_gen.json'\n",
    "        \n",
    "        os.system('rm -r /media/boris/F/Research_final/data/mixer_tts_sup_data_folder')\n",
    "        os.system('mkdir /media/boris/F/Research_final/data/mixer_tts_sup_data_folder')\n",
    "        \n",
    "        self.spec_gen.setup_training_data(DictConfig(d['train_ds']))\n",
    "        \n",
    "    def train_spec_gen(self):\n",
    "    \n",
    "        if self.last_epoch_spec_gen < self.epochs_to_pretrain:\n",
    "            \n",
    "            checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "                dirpath='./spec_gen_saves',\n",
    "                filename='just_last_' + str(self.epochs_to_pretrain),\n",
    "                save_top_k=-1\n",
    "            )\n",
    "            \n",
    "            trainer_sp = pl.Trainer(devices=1, accelerator='gpu', max_epochs=self.epochs_to_pretrain,\n",
    "                                 check_val_every_n_epoch=self.epochs_to_pretrain,\n",
    "                                 num_sanity_val_steps=0,\n",
    "                                 callbacks=[checkpoint_callback],\n",
    "#                                  precision=16,\n",
    "                                 accumulate_grad_batches=1,\n",
    "                                 gradient_clip_val=1000)\n",
    "\n",
    "            self.last_epoch_spec_gen = self.epochs_to_pretrain\n",
    "        \n",
    "        else:\n",
    "            checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "                dirpath='./spec_gen_saves',\n",
    "                filename='just_last_' + str(self.last_epoch_spec_gen),\n",
    "                save_top_k=-1\n",
    "            )\n",
    "            \n",
    "            trainer_sp = pl.Trainer(devices=1, accelerator='gpu', max_epochs=self.last_epoch_spec_gen,\n",
    "                                 check_val_every_n_epoch=1,\n",
    "                                 num_sanity_val_steps=0,\n",
    "                                 callbacks=[checkpoint_callback],\n",
    "                                 resume_from_checkpoint=f'spec_gen_saves/just_last_{self.last_epoch_spec_gen - 1}.ckpt',\n",
    "#                                  precision=16,\n",
    "                                 accumulate_grad_batches=1,\n",
    "                                 gradient_clip_val=1000)\n",
    "        \n",
    "        self.spec_gen.set_trainer(trainer_sp)\n",
    "        trainer_sp.fit(self.spec_gen)\n",
    "            \n",
    "        if self.last_epoch_spec_gen > self.epochs_to_pretrain:\n",
    "            os.system(f'rm spec_gen_saves/just_last_{self.last_epoch_spec_gen - 1}.ckpt')\n",
    "            \n",
    "        self.last_epoch_spec_gen += 1\n",
    "        \n",
    "    def training_epoch_end(self, loss):\n",
    "        \n",
    "        self.last_epoch_asr += 1\n",
    "        \n",
    "        self.cuda()\n",
    "        \n",
    "        if self.last_epoch_asr == self.epochs_to_pretrain:\n",
    "            self.train_spec_gen()\n",
    "        elif self.last_epoch_asr >= self.epochs_to_pretrain:\n",
    "            self.predict_text()\n",
    "            self.train_spec_gen()\n",
    "            self.predict_audio()\n",
    "                  \n",
    "        self.cuda()\n",
    "                \n",
    "        return super().training_epoch_end(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4c6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_dir + 'conf/carnelinet/carnelinet_384.yaml') as f:\n",
    "     asr_cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe1f5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:52:35 mixins:165] Tokenizer SentencePieceTokenizer initialized with 339 tokens\n",
      "[NeMo I 2022-05-26 06:52:35 ctc_bpe_models:246] \n",
      "    Replacing placeholder number of classes (-1) with actual number of classes - 339\n",
      "[NeMo I 2022-05-26 06:52:35 collections:186] Dataset loaded with 800 files totalling 0.62 hours\n",
      "[NeMo I 2022-05-26 06:52:35 collections:187] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2022-05-26 06:52:36 collections:186] Dataset loaded with 100 files totalling 0.08 hours\n",
      "[NeMo I 2022-05-26 06:52:36 collections:187] 0 files were filtered totalling 0.00 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-05-26 06:52:36 ctc_bpe_models:309] Could not load dataset as `manifest_filepath` was None. Provided config : {'manifest_filepath': None, 'sample_rate': 22050, 'batch_size': 32, 'shuffle': False, 'use_start_end_token': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:52:36 features:259] PADDING: 16\n",
      "[NeMo I 2022-05-26 06:52:36 features:276] STFT using torch\n",
      "[NeMo I 2022-05-26 06:52:36 tokenize_and_classify:88] Creating ClassifyFst grammars.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-05-26 06:52:40 g2ps:84] apply_to_oov_word=None, it means that some of words will remain unchanged if they are not handled by one of rule in self.parse_one_word(). It is useful when you use tokenizer with set of phonemes and chars together, otherwise it can be not.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:52:40 data:173] Loading dataset from /media/boris/F/Research_final/data/an4/paired.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [02:23,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:55:03 data:207] Loaded dataset with 800 files.\n",
      "[NeMo I 2022-05-26 06:55:03 data:209] Dataset contains 0.62 hours.\n",
      "[NeMo I 2022-05-26 06:55:03 data:297] Pruned 0 files. Final dataset contains 800 files\n",
      "[NeMo I 2022-05-26 06:55:03 data:299] Pruned 0.00 hours. Final dataset contains 0.62 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:55:03 data:173] Loading dataset from /media/boris/F/Research_final/data/an4/test_manifest.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:17,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:55:21 data:207] Loaded dataset with 100 files.\n",
      "[NeMo I 2022-05-26 06:55:21 data:209] Dataset contains 0.08 hours.\n",
      "[NeMo I 2022-05-26 06:55:21 data:297] Pruned 0 files. Final dataset contains 100 files\n",
      "[NeMo I 2022-05-26 06:55:21 data:299] Pruned 0.00 hours. Final dataset contains 0.08 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:55:21 features:259] PADDING: 1\n",
      "[NeMo I 2022-05-26 06:55:21 features:276] STFT using torch\n",
      "[NeMo I 2022-05-26 06:55:21 cloud:56] Found existing object /home/boris/.cache/torch/NeMo/NeMo_1.8.2/tts_melgan/38f156f172595e60f02169891e303590/tts_melgan.nemo.\n",
      "[NeMo I 2022-05-26 06:55:21 cloud:62] Re-using file from: /home/boris/.cache/torch/NeMo/NeMo_1.8.2/tts_melgan/38f156f172595e60f02169891e303590/tts_melgan.nemo\n",
      "[NeMo I 2022-05-26 06:55:21 common:747] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-05-26 06:55:21 modelPT:148] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.AudioDataset\n",
      "      manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_train.json\n",
      "      max_duration: null\n",
      "      min_duration: 0.75\n",
      "      n_segments: 16384\n",
      "      trim: false\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-05-26 06:55:21 modelPT:155] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.AudioDataset\n",
      "      manifest_filepath: /raid/LJSpeech/nvidia_ljspeech_val.json\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      n_segments: -1\n",
      "      trim: false\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 13\n",
      "      num_workers: 4\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-26 06:55:21 features:259] PADDING: 0\n",
      "[NeMo I 2022-05-26 06:55:21 features:276] STFT using torch\n",
      "[NeMo I 2022-05-26 06:55:21 features:278] STFT using exact pad\n",
      "[NeMo I 2022-05-26 06:55:23 save_restore_connector:209] Model MelGanModel was successfully restored from /home/boris/.cache/torch/NeMo/NeMo_1.8.2/tts_melgan/38f156f172595e60f02169891e303590/tts_melgan.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘tmp’: File exists\n"
     ]
    }
   ],
   "source": [
    "model = SpeechChainModel.from_config_dict(DictConfig(asr_cfg['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236479e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(devices=1, \n",
    "                     accelerator='gpu',\n",
    "                     max_epochs=300,\n",
    "                     check_val_every_n_epoch=1, \n",
    "                     num_sanity_val_steps=0,\n",
    "                     num_nodes=1,\n",
    "                     accumulate_grad_batches=1,\n",
    "#                      precision=16\n",
    "                    )\n",
    "    \n",
    "    \n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
